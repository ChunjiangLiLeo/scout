# simulate_100k.py

import pandas as pd
import numpy as np
import time
import psutil
import os
from sqlalchemy import create_engine
import getpass

# ========== å‚æ•° ==========
target_size = 100_000
chunk_size = 20_000  # åˆ†æ‰¹å†™å…¥
table_name = "transactions_parsed_100k"
csv_path = "transactions_parsed_100k.csv"

# ========== è¾…åŠ©å‡½æ•° ==========
def get_memory_usage():
    process = psutil.Process(os.getpid())
    return round(process.memory_info().rss / (1024 * 1024), 2)

# ========== å¼€å§‹ ==========
start = time.time()
print(f"ğŸš€ å¼€å§‹ç”Ÿæˆ 100,000 æ ·æœ¬ | å½“å‰å†…å­˜ä½¿ç”¨ï¼š{get_memory_usage()} MB")

# è¿æ¥æ•°æ®åº“
username = getpass.getuser()
engine = create_engine(f"postgresql://{username}@localhost:5432/scout_db")

# è¯»å–åŸå§‹æ•°æ®
df = pd.read_sql("SELECT * FROM transactions_parsed", engine)
original_len = len(df)

if original_len == 0:
    print("âŒ åŸå§‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆï¼")
    exit()

# å¤åˆ¶ç”Ÿæˆç›®æ ‡è¡Œæ•°
copies = (target_size // original_len) + 1
values_repeated = np.tile(df.values, (copies, 1))
df_sample = pd.DataFrame(values_repeated, columns=df.columns).head(target_size)
df_sample["id"] = range(1, len(df_sample) + 1)

# åˆ†æ‰¹å†™å…¥ PostgreSQL
print(f"ğŸ“¦ åˆ†æ‰¹å†™å…¥ {table_name}ï¼ˆæ¯æ‰¹ {chunk_size} è¡Œï¼‰")
for i in range(0, len(df_sample), chunk_size):
    df_chunk = df_sample.iloc[i:i + chunk_size]
    print(f"â³ å†™å…¥ç¬¬ {i // chunk_size + 1} æ‰¹ï¼š{len(df_chunk)} è¡Œ")
    df_chunk.to_sql(
        table_name,
        engine,
        if_exists="append" if i > 0 else "replace",
        index=False,
        method='multi',
        chunksize=5000
    )

# å¯é€‰å¯¼å‡ºä¸º CSV
df_sample.to_csv(csv_path, index=False)
print(f"âœ… CSV å·²å¯¼å‡ºè‡³ {csv_path}")

# ========== ç»“æŸ ==========
end = time.time()
print(f"âœ… å†™å…¥å®Œæˆï¼Œå…± {len(df_sample):,} è¡Œ")
print(f"â± æ€»ç”¨æ—¶ï¼š{round(end - start, 2)} ç§’ | å†…å­˜ä½¿ç”¨ï¼š{get_memory_usage()} MB")
