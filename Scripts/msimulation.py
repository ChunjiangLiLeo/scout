# simulate_scale_optimized.py
import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import getpass
import time

start = time.time()
username = getpass.getuser()
engine = create_engine(f"postgresql://{username}@localhost:5432/scout_db")

# Step 1: è¯»å–åŸå§‹æ•°æ®
df = pd.read_sql("SELECT * FROM transactions_parsed", engine)
original_len = len(df)

if original_len == 0:
    print("âŒ åŸå§‹æ•°æ®ä¸ºç©ºï¼Œæ— æ³•ç”Ÿæˆæ ·æœ¬ï¼")
    exit()

# Step 2: å¤åˆ¶æ•°æ® (è¶…å¿«)
target_size = 2_000_000
copies = (target_size // original_len) + 1

print(f"ğŸ§® åŸå§‹è¡Œæ•°ï¼š{original_len}ï¼Œå°†å¤åˆ¶ {copies} æ¬¡ç”Ÿæˆçº¦ 200 ä¸‡æ¡æ ·æœ¬æ•°æ®...")

# ç”¨ numpy.tile å¿«é€Ÿå¤åˆ¶å€¼ï¼ˆæ¯” concat å¿«è‡³å°‘10å€ï¼‰
values_repeated = np.tile(df.values, (copies, 1))
df_sample = pd.DataFrame(values_repeated, columns=df.columns).head(target_size)

# é‡è®¾ ID
df_sample["id"] = range(1, len(df_sample) + 1)

chunk_size = 500_000
for i in range(0, len(df_sample), chunk_size):
    df_chunk = df_sample.iloc[i:i+chunk_size]
    print(f"â³ å†™å…¥ç¬¬ {i//chunk_size + 1} æ‰¹ï¼š{len(df_chunk)} è¡Œ")
    df_chunk.to_sql(
        "transactions_parsed_sample",
        engine,
        if_exists="append" if i > 0 else "replace",
        index=False,
        method='multi',
        chunksize=5000
    )


print(f"âœ… å†™å…¥å®Œæˆï¼Œå…± {len(df_sample):,} è¡Œï¼Œç”¨æ—¶ {round(time.time() - start, 2)} ç§’")


